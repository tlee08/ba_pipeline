{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DLC Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from behavysis_pipeline import *\n",
        "\n",
        "import traceback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "proj_dir = r\".\"\n",
        "\n",
        "proj = BehavysisProject(proj_dir)\n",
        "proj.importExperiments()\n",
        "\n",
        "exp = proj.getExperiments()[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj.updateConfigFile(\n",
        "    default_configs_fp=os.path.join(proj_dir, \"default.json\"),\n",
        "    overwrite=\"set\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj.formatVid(\n",
        "    funcs=(\n",
        "        # formatVid,\n",
        "        getVidMetadata,\n",
        "    ),\n",
        "    overwrite=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj.runDLC(\n",
        "    gputouse=None,\n",
        "    overwrite=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj.calculateParams((\n",
        "    calcStartFrame,\n",
        "    calcEndFrame,\n",
        "    calcPXPerMM,\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj.preprocess(\n",
        "    (\n",
        "        trimToStartEnd,\n",
        "        interpolatePoints,\n",
        "        calcBodyCentre,\n",
        "        refineIdentities,\n",
        "    ),\n",
        "    overwrite=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SIMBA component\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj.extractFeatures(True, True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Classifier from extracted features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ALL saved in `model` folder.\n",
        "Steps:\n",
        "* Data prep\n",
        "    * Make a configs_json, which holds train/test experiment split\n",
        "    * Prepare attributes df in single large DF (extra index midx level \"experiment\")\n",
        "    * Prepare behaviour labels df (each column has a separate classifier though) in single large DF (extra index midx level \"experiment\", and extra column midx level \"outcome\" (\"actual\"))\n",
        "    * Ensure that both X_all and y_all have the same rows (midx is (\"experiment\", \"Frame\"))\n",
        "    * DO THE SAME FOR X_train and X_test SETS. Split videos into subset for each.\n",
        "* Make individual folders for each behaviour we want to train a classifier for (in `behavs_ls`):\n",
        "    * Copy X dfs to each folder\n",
        "    * Save relevant columns of y dfs to each folder\n",
        "    * Copy configs json\n",
        "* For each behav folder, X_all preprocessing\n",
        "    * Random undersampler (select subset of majority class): this seems to work best. Alternatives are random oversampling (repeat minority class instances).\n",
        "        * Undersample X_all and X_train. DO NOT do for X_test.\n",
        "* Define Classifier\n",
        "    * GradientBoost(): This seems to work best. Alternatives are RF, XGBoost, and Keras MLP.\n",
        "* Run Classifier\n",
        "    * From saved classifier hyper-params.\n",
        "* Evaluate\n",
        "    * Using novel videos (from where though?? Maybe save some videos for a X_train and X_test dataset) to create:\n",
        "        * Sorted probability results logistic graph (line for probabilities, points for actuals, vline for threshold).\n",
        "        * Accuracy, Precision, Recall, F1 graph for range of threshold (from 0 to 1).\n",
        "        * Timeseries probabilities against actuals lineplot for each video.\n",
        "        * Annotated video with predicted vs actual behavs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAKING BEHAVS FEATHER DF - SHOULD ONLY BE DONE IN THIS INSTANCE\n",
        "# BECAUSE I'M DERIVING \"fights\" FROM \"green_fights\" and \"white_fights\"\n",
        "# READING BORIS DATA SHOULD ALSO ONLY BE A SPECIAL CASE\n",
        "\n",
        "in_dir = os.path.join(r\"Z:\\PRJ-BowenLab\\TimLee\\resources\\project_ma_new\", \"scored_old\")\n",
        "out_dir = os.path.join(r\"Z:\\PRJ-BowenLab\\TimLee\\resources\\project_ma_new\", \"7_scored_behavs\")\n",
        "for i in os.listdir(in_dir):\n",
        "    name = getName(i)\n",
        "    df = pd.read_csv(os.path.join(in_dir, f\"{name}.csv\"))\n",
        "    for j in [\"white_fights\", \"green_fights\"]:\n",
        "        if j not in df.columns:\n",
        "            df[j] = 0\n",
        "    new_df = pd.DataFrame(index=df[\"Frame\"].astype(np.int64))\n",
        "    # Making new columns\n",
        "    new_df[(\"fight\", BEHAV_ACTUAL_COL)] = ((df[\"green_fights\"] == 1) | (df[\"white_fights\"] == 1)).astype(np.uint8)\n",
        "    new_df[(\"marked_fight\", BEHAV_ACTUAL_COL)] = df[\"green_fights\"]\n",
        "    new_df[(\"unmarked_fight\", BEHAV_ACTUAL_COL)] = df[\"white_fights\"]\n",
        "    # Formatting column names\n",
        "    new_df.columns = pd.MultiIndex.from_tuples(new_df.columns, names=BEHAV_COLUMN_NAMES)\n",
        "    writeFeather(new_df, os.path.join(out_dir, f\"{name}.feather\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_clf = BehavClassifier.from_BehavysisProject(proj)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_clf.combine_dfs(\n",
        "    os.path.join(proj.dir, \"5_features_extracted\"),\n",
        "    os.path.join(proj.dir, \"7_scored_behavs\"),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_clf.make_train_test_split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "behav_clf_ls = []\n",
        "behavs_ls = [\"fight\", \"marked_fight\", \"unmarked_fight\"]\n",
        "for behav in behavs_ls:\n",
        "    behav_clf_ls.append(root_clf.make_behav_model_subdir(behav))\n",
        "\n",
        "# behav_clf_ls = []\n",
        "# behavs_ls = [\"fight\", \"marked_fight\", \"unmarked_fight\"]\n",
        "# for behav in behavs_ls:\n",
        "#     behav_clf_ls.append(BehavClassifier(os.path.join(proj.dir, \"behav_models\", behav, \"model_configs.json\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for behav_clf in behav_clf_ls:\n",
        "    behav_clf.make_random_undersample()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for behav_clf in behav_clf_ls:\n",
        "    behav_clf.init_behav_classifier()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for behav_clf in behav_clf_ls:\n",
        "    behav_clf.train_behav_classifier()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for behav_clf in behav_clf_ls:\n",
        "    behav_clf.model_eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# proj.classifyBehaviour(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: postprocess behav inference (e.g. min-bout duration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj.evaluate(\n",
        "    (\n",
        "        evalVid,\n",
        "        # keypointsPlot\n",
        "    ),\n",
        "    overwrite=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "for i in [\"0_configs\", \"2_formatted_vid\", \"3_dlc_csv\", \"4_preprocessed_csv\", \"5_features_extracted\", \"6_predicted_behavs\", \"analysis\", \"diagnostics\", \"evaluate\"]:\n",
        "    if os.path.exists(os.path.join(proj_dir, i)):\n",
        "        shutil.rmtree(os.path.join(proj_dir, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
